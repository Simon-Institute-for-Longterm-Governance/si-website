---
hide: false
title: Agile multilateral AI governance starts with foundation models
date: 2023-07-05T23:19:53.576Z
featuredImage: usersi_make_it_clearer_its_chess_and_technology_0644d017-47ba-48ef-884e-2587a78779ef.png
authors:
  - name: Konrad Seifert
    page: konrad-seifert/index
  - name: Maxime Stauffer
    page: maxime-stauffer/index
isHighlighted: false
---
### The need to act yet remain agile 

[Artificial intelligence](https://www.simoninstitute.ch/blog/post/artificial-intelligence-a-brief-on-risks-and-opportunities/) is on everyone’s mind – most sectors will be transformed and policymakers seek solutions. At the international level, however, there are significant risks to acting too fast: launching institutions that cannot keep up with technological developments; proposing ideas that alienate future majorities; or competing with more urgent national-level regulatory processes. While the current political attention opens a window of opportunity for AI governance at the multilateral level, we recommend playing the long game.

This governance approach should focus on risks just as much as opportunities. The benefits of AI will be reaped by [tackling risks of injustice just as much as risks of extinction](https://arxiv.org/pdf/2306.06924.pdf), as well as using AI to reduce the digital divide and inequalities more broadly. The key to efficient AI governance is to [focus on the most powerful AI systems](https://arxiv.org/pdf/2108.07258.pdf) – so-called “foundation models”. Similar to Apple or Microsoft with operating systems, these big AI labs develop general-purpose software. Instead of “macOS” and “Windows”, they’re called “GPT-4” or “PaLM 2” and instead of “versions” (e.g. Windows 95 or Vista), they’re called “models” (e.g. GPT-3 and GPT-4). 

This current generation of foundation models has a code base that defines their neural network architecture and learning algorithms, these neural nets then train on vast amounts of data to autonomously develop generally useful behaviors and are iteratively fine-tuned through human feedback. As this development process requires extraordinary amounts of talent, hardware, and data, foundation models can currently be developed by just a few laboratories – notably OpenAI, Google DeepMind, and Anthropic – powered by three big cloud providers: Microsoft Azure, Google Cloud, and Amazon Web Services.

In ideal practice, foundation models would only be made available via an online interface that allows the developers to retain full control and continuously monitor the evolution of the model’s behavior. Making foundation models freely available, as the open source community suggests, would mean that anybody could fine-tune a copy to their liking. This is dangerous not just because the AI could be instructed to achieve illegal ends, but because the models are mostly black boxes – their behavior is plausibly only superficially aligned with human interests. Examples of [well-intentioned AI training gone awry](https://thezvi.substack.com/p/ai-1-sydney-and-bing#%C2%A7the-examples) [exist](https://www.theguardian.com/commentisfree/2023/jun/16/ai-new-laws-powerful-open-source-tools-meta) [aplenty](https://www.forbes.com/sites/bernardmarr/2022/04/01/the-dangers-of-not-aligning-artificial-intelligence-with-human-values/).

As R&D moves faster than the multilateral system can understand and act on, the UN should not seek to spearhead AI governance but act as a guide and mediator – focusing agendas on foundation model regulation and benefit distribution, as well as facilitating the discussions between important stakeholders that otherwise would not happen. This way, the multilateral system can (a) help consolidate and diffuse effective national and regional regulation, and (b) lay the stepping stones for a multilateral regime complex for safe foundation models.

The goal of multilateral activity in the next few years could be to encourage the development of a treaty for responsible AI development in collaboration with key nation-states, aiming for an initial agreement between the two great powers: China and the US. Given the low likelihood of a successful agreement between great powers, a parallel process, including especially the EU and India as crucial mediating parties might allow to build up the necessary pressure and precedent to eventually consolidate an international agreement and according institutions.

The shape of these institutions should not be decided too early. An inclusive and well-mediated stakeholder process would probably lead to the best outcome: an agile regime complex for AI. This might include institutions similar to an IPCC, IAEA, CERN, ICAO or IATA – with varying degrees of enforcement, verification, or standardization powers. But prematurely advancing complex proposals while national legislators are making up their minds is not going to help increase the focus of discussion.  

### Proposals for the 2023 ministerial meeting

The 2023 preparatory ministerial meeting of the Summit of the Future offers an opportunity to advance multilateral AI governance processes. Following the considerations above, we recommend the following actions to be announced at the ministerial meeting: 

1. The appointment of a high-level advisory board on foundation model governance, focused on [analyzing catastrophic risks](https://arxiv.org/pdf/2306.12001.pdf) that would irreversibly shape every current and future person’s life.
2. Developing an inclusive [risk monitoring system](https://www.deepmind.com/blog/an-early-warning-system-for-novel-ai-risks) of AI R&D to foster a global understanding of AI risks and opportunities and the safe deployment of foundation models.
3. A forum to promote the [participatory](https://cip.org/blog/alignment) [governance](https://www.governance.ai/post/what-do-we-mean-when-we-talk-about-ai-democratisation) of AI developments, including the necessary capacity-building support for low- and middle-income countries to develop national expertise.